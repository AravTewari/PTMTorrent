{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from json import dumps\n",
    "from time import time\n",
    "\n",
    "import pandas\n",
    "from time import time\n",
    "from pathlib import PurePath\n",
    "import lxml \n",
    "from bs4 import BeautifulSoup, ResultSet, Tag\n",
    "from requests import Response, get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadPage(url: str = \"https://huggingface.co/organizations?p=0\") -> bytes | int:\n",
    "    resp: Response = get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        return resp.status_code\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageCount(content: bytes) -> int:\n",
    "    pageCount: int = 0\n",
    "\n",
    "    soup: BeautifulSoup = BeautifulSoup(markup=content, features=\"lxml\")\n",
    "    # links: ResultSet = soup.find_all(name=\"a\")\n",
    "    links: ResultSet = soup.find_all(\"a\", {\"class\": \"model-detail-div box-shadow w-100\"})\n",
    "\n",
    "    link: Tag\n",
    "    for link in links:\n",
    "        href: str = link.get(\"href\")\n",
    "        if \"?p=\" in href:\n",
    "            pageCount = int(href[3::]) if int(href[3::]) > pageCount else pageCount\n",
    "\n",
    "    return pageCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveContent(content: bytes, filename: str) -> PurePath:\n",
    "    contentFolderPath: PurePath = PurePath(\"../html/organizations\")\n",
    "    contentFilePath: PurePath = PurePath(os.path.join(contentFolderPath, filename))\n",
    "\n",
    "    with open(contentFilePath, \"wb\") as contentFile:\n",
    "        contentFile.write(content)\n",
    "        contentFile.close()\n",
    "\n",
    "    return contentFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = downloadPage(url=\"https://modelzoo.co/\")\n",
    "content = resp.content\n",
    "text = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://modelzoo.co/\"\n",
    "resp: Response = get(url)\n",
    "resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"downloadPage.txt\", \"w\")\n",
    "file.write(text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "root = soup.find(id='root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "URL = 'https://modelzoo.co/'\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(URL)\n",
    "time.sleep(1) # wait a second for <div id=\"root\"> to be fully loaded\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "driver.quit()\n",
    "root = soup.find(id='root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m link:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mif\u001b[39;00m a[\u001b[39m\"\u001b[39;49m\u001b[39mclass\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m         \u001b[39mprint\u001b[39m(a)\n",
      "File \u001b[0;32m~/anaconda3/envs/PTMTorrent/lib/python3.10/site-packages/bs4/element.py:1519\u001b[0m, in \u001b[0;36mTag.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   1517\u001b[0m     \u001b[39m\"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[39m    and throws an exception if it's not there.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattrs[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "for a in link:\n",
    "    if a[\"class\"] == \"title\":\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model/openpose-caffe\n",
      "/model/mask-r-cnn-keras\n",
      "/model/detectron2\n",
      "/model/ray\n",
      "/model/mmdetection\n",
      "/model/pytorch-cyclegan-and-pix2pix\n",
      "/model/awesome-pytorch-list\n",
      "/model/stylegan\n",
      "/model/pytorch-gan\n",
      "/model/best-of-ml-python\n",
      "/model/fastphotostyle\n",
      "/model/albumentations\n",
      "/model/mathjax\n",
      "/model/maskrcnn-benchmark\n",
      "/model/vid2vid\n",
      "/model/annotated-deep-learning-paper-implementations\n",
      "/model/tensorboard-pytorch\n",
      "/model/deep-image-prior\n",
      "/model/dcgan-tensorflow\n",
      "/model/trax\n",
      "/model/faster-rcnnpytorch\n",
      "/model/yolo-tensorflow\n",
      "/model/tts\n",
      "/model/progressive-growing-of-gans-for-improved-tensorflow\n",
      "/model/pix2pixhd\n",
      "/model/open-source-mit-neural-machine-translation-nmt\n",
      "/model/einops\n",
      "/model/deep-reinforcement-learning-for-keras\n",
      "/model/pytorch-unet-2\n",
      "/model/wavenet-tensorflow\n",
      "/model/yet-another-efficientdet-pytorch\n",
      "/model/external-attention-pytorch\n",
      "/model/stargan-pytorch\n",
      "/model/stargan\n",
      "/model/espnet\n",
      "/model/augmentor\n",
      "/model/sketch-code\n",
      "/model/semantic-segmentation-pytorch\n",
      "/model/deep-reinforcement-learning-algorithms-with-pytorch\n",
      "/model/semantic-segmentation-pytorch-2\n",
      "/model/pytorch-yolov4\n",
      "/model/pytorch-seq2seq-2\n",
      "/model/animegan-tensorflow\n",
      "/model/colornet\n",
      "/model/mmsegmentation\n",
      "/model/image-analogies\n",
      "/model/stable-baselines3\n",
      "/model/flax-60\n",
      "/model/pytorch-semseg\n",
      "/model/fully-convolutional-networks-for-semantic-segmentation-caffe\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")\n",
    "model_links = []\n",
    "for link in links:\n",
    "    href = link.get(\"href\")\n",
    "    model_name = link.get(\"title\")\n",
    "    try:\n",
    "        if \"/model/\"in href and href not in model_links:\n",
    "            model_links.append(href)\n",
    "            pageCount += 1\n",
    "            print(href)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = soup.find_all(\"a\", {\"class\": \"model-detail-div box-shadow w-100\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pageCount: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m soup: BeautifulSoup \u001b[39m=\u001b[39m BeautifulSoup(markup\u001b[39m=\u001b[39;49mcontent, features\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m links \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/PTMTorrent/lib/python3.10/site-packages/bs4/__init__.py:248\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    247\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    249\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    253\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "pageCount: int = 0\n",
    "\n",
    "soup: BeautifulSoup = BeautifulSoup(markup=content, features=\"lxml\")\n",
    "links = soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m link: Tag\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links:\n\u001b[1;32m      3\u001b[0m     href: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m link\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m?p=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m href:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "link: Tag\n",
    "for link in links:  \n",
    "    href: str = link.get(\"href\")\n",
    "    if \"?p=\" in href:\n",
    "        pageCount = int(href[3::]) if int(href[3::]) > pageCount else pageCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTMTorrent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c545c5d350616e5614d4b44e2f21515d1f34854aa72a6e8c034618909fce7af6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
